import os
import weaviate
import streamlit as st
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

# --- LangChain/Weaviate Imports ---
from weaviate.classes.init import Auth
from weaviate.classes.query import Filter
from weaviate.collections import Collection
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_classic.chains.retrieval_qa.base import RetrievalQA
from typing import List, Dict, Any, Optional

# --- Import Predictor ---
# ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n v√† t√™n bi·∫øn export trong predict.py l√† ƒë√∫ng
import sys
sys.path.append(os.path.join(os.getcwd(), 'document_classification'))
try:
    from document_classification.predict import DocumentPredictor 
except ImportError:
    st.error("L·ªói Import: Vui l√≤ng ƒë·∫£m b·∫£o file 'document_classification/predict.py' t·ªìn t·∫°i.")
    DocumentPredictor = None

# ----------------------------------------------------------------------
# 1. H√ÄM V√Ä L·ªöP LOGIC RAG
# ----------------------------------------------------------------------

def reciprocal_rank_fusion(results: List[Any], k: int = 60) -> List[Dict[str, Any]]:
    fused_scores = {}
    for result_list in results:
        for rank, obj in enumerate(result_list):
            uuid = str(obj.uuid)
            score = 1.0 / (k + rank + 1)
            if uuid not in fused_scores:
                fused_scores[uuid] = {"score": 0.0, "object": obj}
            fused_scores[uuid]["score"] += score
    sorted_fused_results = sorted(
        fused_scores.values(), 
        key=lambda x: x["score"], 
        reverse=True
    )
    return sorted_fused_results

class ConfigurableHybridRetriever(BaseRetriever):
    history_collection: Collection 
    embed_model: SentenceTransformer
    k: int = 4
    search_mode: str
    use_filter: bool

    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:
        
        where_filter: Optional[Filter] = None
        predicted_period: str = "Kh√¥ng l·ªçc"
        
        if self.use_filter and DocumentPredictor:
            predicted_period = DocumentPredictor.predict(query)
            where_filter = Filter.by_property("period").equal(predicted_period)
            
        query_vector = self.embed_model.encode(query).tolist() 
        vector_results_objects = []
        bm25_results_objects = []

        if self.search_mode in ['semantic', 'hybrid']:
            vector_results = self.history_collection.query.near_vector(
                near_vector=query_vector,
                limit=self.k * 3 if self.search_mode == 'hybrid' else self.k,
                return_properties=["context", "period"],
                filters=where_filter,
            )
            vector_results_objects = vector_results.objects
            
        if self.search_mode in ['keyword', 'hybrid']:
            bm25_results = self.history_collection.query.bm25(
                query=query,
                limit=self.k * 3 if self.search_mode == 'hybrid' else self.k,
                return_properties=["context", "period"],
                filters=where_filter,
            )
            bm25_results_objects = bm25_results.objects

        final_results = []
        
        if self.search_mode == 'hybrid':
            fused_objects = reciprocal_rank_fusion([vector_results_objects, bm25_results_objects], k=60)
            final_results = fused_objects[:self.k]
        elif self.search_mode == 'semantic':
            final_results = [{"score": 1.0, "object": obj} for obj in vector_results_objects[:self.k]]
        elif self.search_mode == 'keyword':
            final_results = [{"score": 1.0, "object": obj} for obj in bm25_results_objects[:self.k]]

        documents = []
        for item in final_results:
            obj = item["object"]
            metadata = {
                "period": obj.properties.get("period", "N/A"),
                "source_uuid": str(obj.uuid),
                "rrf_score": item["score"],
                "predicted_period": predicted_period 
            }
            documents.append(
                Document(page_content=obj.properties.get("context", ""), metadata=metadata)
            )
        
        return documents

# ----------------------------------------------------------------------
# 2. KH·ªûI T·∫†O V√Ä C·∫§U H√åNH STREAMLIT
# ----------------------------------------------------------------------

load_dotenv() 

def setup_rag_system(temperature: float, k_value: int, search_mode: str, use_filter: bool):
    """Th·ª±c hi·ªán kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn RAG."""
    
    WEAVIATE_URL = os.environ.get("WEAVIATE_URL")
    WEAVIATE_API_KEY = os.environ.get("WEAVIATE_API_KEY")
    GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY") 
    COLLECTION_NAME = "History"
    LOCAL_MODEL_PATH = os.environ.get("EMBEDDING_MODEL_NAME")

    embed_model = SentenceTransformer(LOCAL_MODEL_PATH)
    
    client = weaviate.connect_to_weaviate_cloud(
        cluster_url=WEAVIATE_URL,
        auth_credentials=Auth.api_key(WEAVIATE_API_KEY),
    )
    
    if not client.is_connected():
         client = weaviate.connect_to_weaviate_cloud(
            cluster_url=WEAVIATE_URL,
            auth_credentials=Auth.api_key(WEAVIATE_API_KEY),
        )

    history_collection = client.collections.get(COLLECTION_NAME)

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash", 
        google_api_key=GEMINI_API_KEY,
        temperature=temperature,
    )

    retriever = ConfigurableHybridRetriever(
        history_collection=history_collection,
        embed_model=embed_model,
        k=k_value,
        search_mode=search_mode,
        use_filter=use_filter,
    )

    template = """B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh v·ªÅ L·ªãch s·ª≠ Vi·ªát Nam. 
    H√£y s·ª≠ d·ª•ng c√°c ƒëo·∫°n ng·ªØ c·∫£nh (Context) ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch chi ti·∫øt v√† trung th·ª±c. 
    
    Ng·ªØ c·∫£nh: {context}
    C√¢u h·ªèi: {question}
    C√¢u tr·∫£ l·ªùi chi ti·∫øt:
    """
    RAG_PROMPT_CUSTOM = PromptTemplate.from_template(template)

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff", 
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": RAG_PROMPT_CUSTOM}
    )
    
    return qa_chain, client

def initialize_rag_system(temp, k, mode, filter):
    """Th·ª±c hi·ªán kh·ªüi t·∫°o h·ªá th·ªëng RAG v√† l∆∞u v√†o Session State."""
    try:
        with st.spinner("‚è≥ ƒêang Kh·ªüi t·∫°o H·ªá th·ªëng RAG..."):
            qa_chain, client = setup_rag_system(temp, k, mode, filter)
            
            st.session_state['qa_chain'] = qa_chain
            st.session_state['weaviate_client'] = client
            st.session_state['rag_initialized'] = True
            st.session_state['last_config'] = (temp, k, mode, filter)
        
        st.success("‚úÖ H·ªá th·ªëng RAG ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng! B·∫°n c√≥ th·ªÉ ƒë·∫∑t c√¢u h·ªèi.")
        st.session_state.current_config_status = "ƒê√£ kh·ªüi t·∫°o"
        
    except Exception as e:
        st.error(f"‚ùå L·ªói Kh·ªüi t·∫°o: {e}")
        st.session_state['rag_initialized'] = False
        st.session_state.current_config_status = "L·ªói kh·ªüi t·∫°o"

# ----------------------------------------------------------------------
# 3. GIAO DI·ªÜN STREAMLIT
# ----------------------------------------------------------------------

st.set_page_config(page_title="üáªüá≥ Chatbot L·ªãch s·ª≠ Vi·ªát Nam (RAG)", layout="wide")

# Y√™u c·∫ßu 2: Thay ƒë·ªïi ti√™u ƒë·ªÅ ch√≠nh
st.title("Chatbot L·ªãch s·ª≠ Vi·ªát Nam (RAG)")
st.caption("S·ª≠ d·ª•ng Gemini 2.5 Flash, Hybrid Search (RRF) & Ph√¢n lo·∫°i T√†i li·ªáu")

# --- KH·ªûI T·∫†O TR·∫†NG TH√ÅI CU·ªòC TR√í CHUY·ªÜN & C·∫§U H√åNH ---
if 'rag_initialized' not in st.session_state:
    st.session_state['rag_initialized'] = False
    st.session_state.messages = []
    st.session_state.current_config_status = "Ch∆∞a kh·ªüi t·∫°o"
    st.session_state.last_config = (0.1, 4, 'hybrid', True) # Gi√° tr·ªã m·∫∑c ƒë·ªãnh

# --- C·ªôt b√™n (Sidebar) ---
with st.sidebar:
    # Y√™u c·∫ßu 1: Thay ƒë·ªïi ti√™u ƒë·ªÅ sidebar
    st.header("ViHistory Chatbot")

    temperature = st.slider("1Ô∏è‚É£ ƒê·ªô s√°ng t·∫°o (Temperature)", min_value=0.0, max_value=1.0, value=st.session_state.last_config[0], step=0.05, key="temp_slider")
    k_value = st.slider("2Ô∏è‚É£ S·ªë l∆∞·ª£ng Context (K)", min_value=1, max_value=10, value=st.session_state.last_config[1], step=1, key="k_slider")
    search_mode = st.radio(
        "3Ô∏è‚É£ Ph∆∞∆°ng ph√°p Truy xu·∫•t", 
        ('hybrid', 'semantic', 'keyword'),
        format_func=lambda x: {'semantic': 'Ng·ªØ nghƒ©a (Vector)', 'keyword': 'T·ª´ kh√≥a (BM25)', 'hybrid': 'K·∫øt h·ª£p (RRF)'}[x],
        index=['hybrid', 'semantic', 'keyword'].index(st.session_state.last_config[2]),
        key="search_mode_radio"
    )
    # Y√™u c·∫ßu 3: Thay ƒë·ªïi t√™n checkbox
    use_filter = st.checkbox("4Ô∏è‚É£ B·ªô l·ªçc c√¢u h·ªèi", value=st.session_state.last_config[3], key="filter_checkbox")
    
    st.markdown("---")
    
    # Y√™u c·∫ßu 4: Hi·ªÉn th·ªã tr·∫°ng th√°i c·∫•u h√¨nh
    st.metric("Tr·∫°ng th√°i H·ªá th·ªëng", st.session_state.current_config_status)
    st.caption("Vui l√≤ng kh·ªüi t·∫°o l·∫°i h·ªá th·ªëng khi thay ƒë·ªïi c√°c t√πy ch·ªçn!") 

    if st.button("‚öôÔ∏è Kh·ªüi t·∫°o H·ªá th·ªëng RAG"):
        initialize_rag_system(temperature, k_value, search_mode, use_filter)


# --- Khu v·ª±c Chat Ch√≠nh ---

# 1. Hi·ªÉn th·ªã L·ªãch s·ª≠ Chat
for message in st.session_state.messages:
    # Y√™u c·∫ßu 5: ƒê·∫£m b·∫£o ngu·ªìn xu·∫•t hi·ªán ngay l·∫≠p t·ª©c
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
        # N·∫øu l√† Assistant, hi·ªÉn th·ªã ngu·ªìn ngay b√™n d∆∞·ªõi c√¢u tr·∫£ l·ªùi
        if message["role"] == "assistant" and "sources" in message:
            
            # Hi·ªÉn th·ªã th√¥ng tin L·ªçc (n·∫øu c√≥)
            if message['filter_info']:
                st.info(message['filter_info'])
                
            # M·ªü expander cho Ngu·ªìn Context
            # with st.expander(f"üìö Xem {len(message['sources'])} ƒêo·∫°n Ngu·ªìn Context"):
            #     for i, doc in enumerate(message['sources']):
            #         metadata = doc.metadata
                    
            #         # Y√™u c·∫ßu 6: Lo·∫°i b·ªè Ngu·ªìn UUID v√† RRF Score
            #         st.markdown(f"**T√†i li·ªáu {i+1}: {metadata.get('period', 'N/A')}**")
            #         st.code(doc.page_content, language='markdown')


# 2. Nh·∫≠p C√¢u h·ªèi m·ªõi (Chat Input)
if st.session_state['rag_initialized']:
    
    user_query = st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ L·ªãch s·ª≠ Vi·ªát Nam...")

    if user_query:
        # A. Th√™m c√¢u h·ªèi ng∆∞·ªùi d√πng v√†o l·ªãch s·ª≠
        st.session_state.messages.append({"role": "user", "content": user_query})
        # Hi·ªÉn th·ªã ngay l·∫≠p t·ª©c ƒë·ªÉ t·∫°o hi·ªáu ·ª©ng chat
        with st.chat_message("user"):
            st.markdown(user_query)

        # B. L·∫•y c√¢u tr·∫£ l·ªùi v√† hi·ªÉn th·ªã
        with st.chat_message("assistant"):
            with st.spinner("ü§ñ ƒêang x·ª≠ l√Ω..."):
                try:
                    qa_chain = st.session_state['qa_chain']
                    result = qa_chain.invoke({"query": user_query}) 

                    response = result["result"]
                    source_documents = result["source_documents"]
                    
                    # Chu·∫©n b·ªã th√¥ng tin l·ªçc ƒë·ªÉ hi·ªÉn th·ªã trong message (Y√™u c·∫ßu 2: Kh√¥ng hi·ªÉn th·ªã khi Filter T·∫ÆT)
                    filter_info = None
                    if source_documents and qa_chain.retriever.use_filter:
                        predicted = source_documents[0].metadata.get('predicted_period', 'N/A')
                        filter_info = f"üî• **ƒê√£ L·ªçc theo Ch·ªß ƒë·ªÅ:** **{predicted}** (D·ª± ƒëo√°n t·ª´ c√¢u h·ªèi)"

                    # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi (sau khi x·ª≠ l√Ω)
                    st.markdown(response)

                    if source_documents:
                        # Logic hi·ªÉn th·ªã filter_info v√† expander cho sources
                        if filter_info:
                            st.info(filter_info)
                        
                        with st.expander(f"üìö Xem {len(source_documents)} ƒêo·∫°n Ngu·ªìn Context"):
                            for i, doc in enumerate(source_documents):
                                metadata = doc.metadata
                                st.markdown(f"**T√†i li·ªáu {i+1}: {metadata.get('period', 'N/A')}**")
                                st.code(doc.page_content, language='markdown')
                    
                    # C. Th√™m c√¢u tr·∫£ l·ªùi v√† ngu·ªìn v√†o l·ªãch s·ª≠
                    st.session_state.messages.append({
                        "role": "assistant", 
                        "content": response, 
                        "sources": source_documents,
                        "filter_info": filter_info
                    })
                    
                except Exception as e:
                    error_message = f"‚ùå ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh RAG: {e}"
                    st.error(error_message)
                    st.session_state.messages.append({"role": "assistant", "content": error_message})
else:
    # Hi·ªÉn th·ªã th√¥ng b√°o khi ch∆∞a kh·ªüi t·∫°o
    st.info("Vui l√≤ng c·∫•u h√¨nh c√°c t√πy ch·ªçn v√† nh·∫•n **'‚öôÔ∏è Kh·ªüi t·∫°o H·ªá th·ªëng RAG'** ·ªü sidebar ƒë·ªÉ b·∫Øt ƒë·∫ßu.")