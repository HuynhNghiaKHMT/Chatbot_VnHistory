import os
import streamlit as st
import time
import uuid
from datetime import datetime
from configparser import ConfigParser
from typing import List, Dict, Any, Optional

# --- CORE LOGIC IMPORTS (Weaviate/LangChain/Embedding) ---
import weaviate
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

from weaviate.classes.init import Auth
from weaviate.classes.query import Filter
from weaviate.collections import Collection
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_classic.chains.retrieval_qa.base import RetrievalQA

# --- Import Predictor (Gi·ªØ nguy√™n logic c·ªßa b·∫°n) ---
import sys
# Th√™m th∆∞ m·ª•c document_classification v√†o path
sys.path.append(os.path.join(os.getcwd(), 'document_classification')) 
try:
    from document_classification.predict import DocumentPredictor 
except ImportError:
    # print("WARNING: L·ªói Import DocumentPredictor. Ch·ª©c nƒÉng l·ªçc s·∫Ω kh√¥ng ho·∫°t ƒë·ªông.")
    DocumentPredictor = None

# --- LOAD ENV V√Ä CONFIG ---
load_dotenv() 
config = ConfigParser()
try:
    config.read(f"{os.path.dirname(os.path.abspath(__file__))}/env.ini")
except Exception:
    pass 

# ----------------------------------------------------------------------
# 1. CORE RAG LOGIC (ConfigurableHybridRetriever v√† RRF)
# ----------------------------------------------------------------------

def reciprocal_rank_fusion(results: List[Any], k: int = 60) -> List[Dict[str, Any]]:
    # H√†m RRF c·ªßa b·∫°n
    fused_scores = {}
    for result_list in results:
        for rank, obj in enumerate(result_list):
            uuid = str(obj.uuid)
            score = 1.0 / (k + rank + 1)
            if uuid not in fused_scores:
                fused_scores[uuid] = {"score": 0.0, "object": obj}
            fused_scores[uuid]["score"] += score
    sorted_fused_results = sorted(
        fused_scores.values(), 
        key=lambda x: x["score"], 
        reverse=True
    )
    return sorted_fused_results

class ConfigurableHybridRetriever(BaseRetriever):
    history_collection: Collection 
    embed_model: SentenceTransformer
    k: int = 4
    search_mode: str
    use_filter: bool

    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:
        # Logic truy xu·∫•t Weaviate c·ªßa b·∫°n
        where_filter: Optional[Filter] = None
        predicted_period: str = "Kh√¥ng l·ªçc"
        
        if self.use_filter and DocumentPredictor:
              try:
                predicted_period = DocumentPredictor.predict(query)
                where_filter = Filter.by_property("period").equal(predicted_period)
              except Exception as e:
                print(f"L·ªói khi d·ª± ƒëo√°n/l·ªçc: {e}")
                predicted_period = "L·ªói d·ª± ƒëo√°n"
        
        query_vector = self.embed_model.encode(query).tolist() 
        vector_results_objects = []
        bm25_results_objects = []

        if self.search_mode in ['semantic', 'hybrid']:
            vector_results = self.history_collection.query.near_vector(
                near_vector=query_vector,
                limit=self.k * 3 if self.search_mode == 'hybrid' else self.k,
                return_properties=["context", "period"],
                filters=where_filter,
            )
            vector_results_objects = vector_results.objects
            
        if self.search_mode in ['keyword', 'hybrid']:
            bm25_results = self.history_collection.query.bm25(
                query=query,
                limit=self.k * 3 if self.search_mode == 'hybrid' else self.k,
                return_properties=["context", "period"],
                filters=where_filter,
            )
            bm25_results_objects = bm25_results.objects

        final_results = []
        if self.search_mode == 'hybrid':
            fused_objects = reciprocal_rank_fusion([vector_results_objects, bm25_results_objects], k=60)
            final_results = fused_objects[:self.k]
        elif self.search_mode == 'semantic':
            final_results = [{"score": 1.0, "object": obj} for obj in vector_results_objects[:self.k]]
        elif self.search_mode == 'keyword':
            final_results = [{"score": 1.0, "object": obj} for obj in bm25_results_objects[:self.k]]

        documents = []
        for item in final_results:
            obj = item["object"]
            metadata = {
                "period": obj.properties.get("period", "N/A"),
                "source_uuid": str(obj.uuid),
                "predicted_period": predicted_period 
            }
            documents.append(
                Document(page_content=obj.properties.get("context", ""), metadata=metadata)
            )
        
        return documents
    
    # Ph∆∞∆°ng th·ª©c m·ªõi ƒë·ªÉ g·ªçi RAG Chain
    def ask(self, query: str) -> tuple[str, List[Document], Optional[str]]:
        qa_chain = st.session_state['qa_chain']
        result = qa_chain.invoke({"query": query})
        
        response = result["result"]
        source_documents = result["source_documents"]
        
        # Chu·∫©n b·ªã th√¥ng tin l·ªçc
        filter_info = None
        if source_documents and self.use_filter:
            predicted = source_documents[0].metadata.get('predicted_period', 'N/A')
            filter_info = f"üî• **ƒê√£ L·ªçc theo Ch·ªß ƒë·ªÅ:** **{predicted}** (D·ª± ƒëo√°n t·ª´ c√¢u h·ªèi)"
            
        return response, source_documents, filter_info


def setup_rag_system(temperature: float, k_value: int, search_mode: str, use_filter: bool):
    """Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn RAG v√† tr·∫£ v·ªÅ qa_chain."""
    
    WEAVIATE_URL = os.environ.get("WEAVIATE_URL")
    WEAVIATE_API_KEY = os.environ.get("WEAVIATE_API_KEY")
    GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY") 
    COLLECTION_NAME = "History"
    LOCAL_MODEL_PATH = os.environ.get("EMBEDDING_MODEL_NAME")

    if not all([WEAVIATE_URL, WEAVIATE_API_KEY, GEMINI_API_KEY, LOCAL_MODEL_PATH]):
        raise EnvironmentError("Thi·∫øu bi·∫øn m√¥i tr∆∞·ªùng (WEAVIATE_URL, WEAVIATE_API_KEY, GEMINI_API_KEY, EMBEDDING_MODEL_NAME)!")

    # Ch·∫Øc ch·∫Øn r·∫±ng m√¥ h√¨nh Embedding ƒë√£ ƒë∆∞·ª£c t·∫£i v·ªÅ (ho·∫∑c c√≥ th·ªÉ d√πng try/except)
    embed_model = SentenceTransformer(LOCAL_MODEL_PATH)
    
    client = weaviate.connect_to_weaviate_cloud(
        cluster_url=WEAVIATE_URL,
        auth_credentials=Auth.api_key(WEAVIATE_API_KEY),
    )
    history_collection = client.collections.get(COLLECTION_NAME)

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash", 
        google_api_key=GEMINI_API_KEY,
        temperature=temperature,
    )

    retriever = ConfigurableHybridRetriever(
        history_collection=history_collection,
        embed_model=embed_model,
        k=k_value,
        search_mode=search_mode,
        use_filter=use_filter,
    )

    template = """B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh v·ªÅ L·ªãch s·ª≠ Vi·ªát Nam. 
    H√£y s·ª≠ d·ª•ng c√°c ƒëo·∫°n ng·ªØ c·∫£nh (Context) ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch chi ti·∫øt v√† trung th·ª±c. 
    N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin trong ng·ªØ c·∫£nh, h√£y tr·∫£ l·ªùi l√† 'T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong ngu·ªìn c·∫•p.'
    
    Ng·ªØ c·∫£nh: {context}
    C√¢u h·ªèi: {question}
    C√¢u tr·∫£ l·ªùi chi ti·∫øt:
    """
    RAG_PROMPT_CUSTOM = PromptTemplate.from_template(template)

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff", 
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": RAG_PROMPT_CUSTOM}
    )
    
    return qa_chain, retriever, client

@st.cache_resource
def initialize_rag_system(temp, k, mode, filter, show_sources): # TH√äM tham s·ªë show_sources
    """Th·ª±c hi·ªán kh·ªüi t·∫°o h·ªá th·ªëng RAG v√† l∆∞u v√†o Session State."""
    try:
        with st.spinner("‚è≥ ƒêang Kh·ªüi t·∫°o H·ªá th·ªëng RAG..."):
            qa_chain, retriever, client = setup_rag_system(temp, k, mode, filter)
            
            st.session_state['qa_chain'] = qa_chain
            st.session_state['rag_retriever'] = retriever
            st.session_state['weaviate_client'] = client
            st.session_state['rag_initialized'] = True
            # L∆ØU 5 GI√Å TR·ªä V√ÄO last_config
            st.session_state['last_config'] = (temp, k, mode, filter, show_sources) 
            st.session_state.current_config_status = "ƒê√£ kh·ªüi t·∫°o"
        
        st.success("‚úÖ H·ªá th·ªëng RAG ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng!")
        return True
    
    except Exception as e:
        st.session_state['rag_initialized'] = False
        st.session_state.current_config_status = "L·ªói kh·ªüi t·∫°o"
        st.error(f"‚ùå L·ªói Kh·ªüi t·∫°o: {e}")
        return False
    
# ----------------------------------------------------------------------
# 2. FRONTEND LOGIC & C·∫§U H√åNH BAN ƒê·∫¶U
# ----------------------------------------------------------------------

st.set_page_config(
    page_title="3NHistory | Vietnam History AI",
    page_icon="üáªüá≥",
    layout="wide",
    initial_sidebar_state="expanded" 
)

# --- KH·ªûI T·∫†O TR·∫†NG TH√ÅI CU·ªòC TR√í CHUY·ªÜN & C·∫§U H√åNH ---
if 'messages' not in st.session_state:
    st.session_state.messages = []
if 'total_questions' not in st.session_state:
    st.session_state.total_questions = 0
if 'show_sources' not in st.session_state:
    st.session_state.show_sources = True 
if 'k_value' not in st.session_state:
    st.session_state.k_value = 5 
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'current_chat_id' not in st.session_state:
    st.session_state.current_chat_id = None
if 'rag_initialized' not in st.session_state:
    st.session_state['rag_initialized'] = False
    st.session_state.current_config_status = "Ch∆∞a kh·ªüi t·∫°o"
    st.session_state.last_config = (0.1, 4, 'hybrid', True, True) # Th√™m show_sources v√†o config
    st.session_state['rag_retriever'] = None
    st.session_state['qa_chain'] = None
# BI·∫æN TR·∫†NG TH√ÅI M·ªöI ƒê·ªÇ K√çCH HO·∫†T RAG SAU KHI RERUN (Gi·∫£i ph√°p cho vi·ªác hi·ªÉn th·ªã ngay l·∫≠p t·ª©c)
if 'process_rag' not in st.session_state:
    st.session_state.process_rag = False

if 'temp_slider' not in st.session_state:
    st.session_state.temp_slider = st.session_state.last_config[0]
    st.session_state.k_slider = st.session_state.last_config[1]
    st.session_state.search_mode_radio = st.session_state.last_config[2]
    st.session_state.filter_checkbox = st.session_state.last_config[3]
    st.session_state.show_sources_checkbox = st.session_state.last_config[4]


# --- CSS - Dark Mode Modern Design (ƒê√£ th√™m CSS kh√≥a Sidebar) ---
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap');
    
    * { font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif; }
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    .main {
        background: linear-gradient(135deg, #0f172a 0%, #1e1b4b 50%, #312e81 100%);
    }
    
    /* Top Bar */
    .top-bar {
        position: fixed; top: 0; left: 0; right: 0; height: 70px; display: flex; 
        align-items: center; justify-content: space-between; padding: 0 2rem;
        z-index: 1000; background: rgba(15, 23, 42, 0.95);
        backdrop-filter: blur(20px); border-bottom: 1px solid rgba(139, 92, 246, 0.2);
        box-shadow: 0 4px 30px rgba(0, 0, 0, 0.3);
    }
    .top-bar-left { display: flex; align-items: center; gap: 1rem; }
    .stats-badge {
        background: rgba(139, 92, 246, 0.2); border: 1px solid rgba(139, 92, 246, 0.3);
        border-radius: 20px; padding: 0.5rem 1rem; color: #c4b5fd;
        font-size: 0.875rem; font-weight: 600;
    }
    
    /* Chat Container */
    .chat-container {
        max-width: 900px; margin: 90px auto 120px auto; padding: 0 1.5rem;
    }
    
    /* Welcome Screen */
    .welcome-container {
        max-width: 1000px; margin: 100px auto 0 auto; padding: 2rem 1.5rem;
        text-align: center; animation: fadeIn 0.8s ease;
    }
    @keyframes fadeIn { from { opacity: 0; transform: translateY(30px); } to { opacity: 1; transform: translateY(0); } }
    
    .welcome-title {
        font-size: 3.5rem; font-weight: 900;
        background: linear-gradient(135deg, #8b5cf6 0%, #ec4899 50%, #f59e0b 100%);
        -webkit-background-clip: text; -webkit-text-fill-color: transparent;
        margin-bottom: 1rem; animation: glow 3s ease-in-out infinite;
    }
    @keyframes glow { 0%, 100% { filter: drop-shadow(0 0 20px rgba(139, 92, 246, 0.4)); } 50% { filter: drop-shadow(0 0 40px rgba(236, 72, 153, 0.6)); } }
    
    .suggestion-grid {
        display: grid; grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
        gap: 1.5rem; margin-top: 3rem;
    }
    
    /* Source Card */
    .source-card {
        background: rgba(30, 41, 59, 0.6); border: 1px solid rgba(139, 92, 246, 0.3);
        border-radius: 12px; padding: 1rem; margin: 0.5rem 0;
        transition: all 0.3s ease; backdrop-filter: blur(10px);
    }
    
    /* Sidebar (ƒê√£ th√™m CSS kh√≥a Sidebar) */
    /* ·∫®n n√∫t Hamburger menu/Collapse ƒë·ªÉ ngƒÉn ƒë√≥ng sidebar */
    [data-testid="stSidebarToggleButton"] {
        visibility: hidden;
    }
    
    section[data-testid="stSidebar"] {
        background: rgba(15, 23, 42, 0.95) !important;
        border-right: 1px solid rgba(139, 92, 246, 0.2) !important;
        backdrop-filter: blur(20px);
        
        /* Kh√≥a k√≠ch th∆∞·ªõc sidebar */
        width: 300px !important; 
        min-width: 300px !important;
        max-width: none !important; 
        transform: none !important;
    }
    section[data-testid="stSidebar"] * { color: #e5e7eb !important; }
    section[data-testid="stSidebar"] h3 { color: #c4b5fd !important; }
    section[data-testid="stSidebar"] .stButton button {
        width: 100% !important; border-radius: 12px !important; padding: 0.75rem 1rem !important;
        background: linear-gradient(135deg, #8b5cf6 0%, #7c3aed 100%) !important;
        border: none !important; color: white !important; font-weight: 600 !important; 
        box-shadow: 0 4px 15px rgba(139, 92, 246, 0.3) !important;
    }
    
    /* Chat Messages */
    [data-testid="stChatMessage"] {
        background: rgba(30, 41, 59, 0.5) !important;
        border: 1px solid rgba(139, 92, 246, 0.2) !important;
        border-radius: 16px !important; backdrop-filter: blur(10px) !important;
        margin-bottom: 1rem !important; animation: slideIn 0.4s ease !important;
    }
</style>
""", unsafe_allow_html=True)


# --- Helper Functions (Gi·ªØ nguy√™n) ---
def save_current_chat():
    """L∆∞u cu·ªôc h·ªôi tho·∫°i hi·ªán t·∫°i"""
    if st.session_state.messages and st.session_state.current_chat_id is None:
        chat_id = str(uuid.uuid4())
        first_message = next((msg['content'] for msg in st.session_state.messages if msg['role'] == 'user'), "H·ªôi tho·∫°i m·ªõi")
        
        st.session_state.chat_history.append({
            'id': chat_id,
            'title': first_message[:50].strip() + ("..." if len(first_message) > 50 else ""),
            'messages': st.session_state.messages.copy(),
            'timestamp': datetime.now().strftime("%H:%M %d/%m")
        })

def load_chat(chat_id):
    """Load m·ªôt cu·ªôc h·ªôi tho·∫°i c≈©"""
    save_current_chat()
    for chat in st.session_state.chat_history:
        if chat['id'] == chat_id:
            st.session_state.messages = chat['messages'].copy()
            st.session_state.current_chat_id = chat_id
            break
        
def delete_chat(chat_id):
    """X√≥a m·ªôt cu·ªôc h·ªôi tho·∫°i"""
    st.session_state.chat_history = [c for c in st.session_state.chat_history if c['id'] != chat_id]
    if st.session_state.current_chat_id == chat_id:
        st.session_state.messages = []
        st.session_state.current_chat_id = None

def new_chat():
    """T·∫°o cu·ªôc h·ªôi tho·∫°i m·ªõi"""
    save_current_chat()
    st.session_state.messages = []
    st.session_state.current_chat_id = None


# ----------------------------------------------------------------------
# 3. GIAO DI·ªÜN CH√çNH (SIDEBAR, TOP BAR, CHAT AREA)
# ----------------------------------------------------------------------

# --- Sidebar ---
with st.sidebar:
    # Logo v√† m√¥ t·∫£
    st.markdown(f"""
        <div style="text-align: center; margin-bottom: 1rem; padding: 10px 0;">
            <img src="[YOUR_PUBLIC_LOGO_URL]" alt="3NHistory Logo" style="height: 45px; margin-bottom: 0.5rem;"/>
            <p style="font-size: 0.9rem; color: #a0aec0; margin: 0;">Vietnam History AI Assistant</p>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    if st.button("‚ûï Cu·ªôc h·ªôi tho·∫°i m·ªõi", use_container_width=True):
        new_chat()
        st.rerun()
    
    st.markdown("---")

    # A. C·∫•u h√¨nh RAG (ƒê√£ c·∫≠p nh·∫≠t th·ª© t·ª± v√† th√¥ng b√°o)
    st.markdown("### ‚öôÔ∏è C·∫•u h√¨nh RAG")
    
    temperature = st.slider("1Ô∏è‚É£ ƒê·ªô s√°ng t·∫°o (Temperature)", min_value=0.0, max_value=1.0, 
                            step=0.05, key="temp_slider") 
    
    k_value = st.slider("2Ô∏è‚É£ S·ªë l∆∞·ª£ng Context (K)", min_value=1, max_value=10, 
                            step=1, key="k_slider") 
    
    search_mode = st.radio(
        "3Ô∏è‚É£ Ph∆∞∆°ng ph√°p Truy xu·∫•t", 
        ('hybrid', 'semantic', 'keyword'),
        format_func=lambda x: {'semantic': 'Ng·ªØ nghƒ©a', 'keyword': 'T·ª´ kh√≥a', 'hybrid': 'K·∫øt h·ª£p'}[x],
        key="search_mode_radio"
    )
    
    use_filter = st.checkbox("4Ô∏è‚É£ B·ªô l·ªçc c√¢u h·ªèi", 
                             key="filter_checkbox", 
                             help="S·ª≠ d·ª•ng m√¥ h√¨nh ph√¢n lo·∫°i ƒë·ªÉ l·ªçc t√†i li·ªáu theo th·ªùi k·ª≥.")
    
    st.session_state.show_sources = st.checkbox(
        "5Ô∏è‚É£ Hi·ªÉn th·ªã ngu·ªìn tham kh·∫£o", 
        key="show_sources_checkbox"
    )
        
    # Y√™u c·∫ßu 1: X√≥a g·∫°ch d∆∞·ªõi tr∆∞·ªõc n√∫t Kh·ªüi t·∫°o RAG
    st.markdown("---")
    
    # Y√™u c·∫ßu 2: Thay ƒë·ªïi th√¥ng b√°o tr·∫°ng th√°i
    if st.session_state.current_config_status == "ƒê√£ kh·ªüi t·∫°o":
        status_message = "H√£y kh·ªüi t·∫°o l·∫°i RAG n·∫øu b·∫°n thay ƒë·ªïi c√°c t√πy ch·ªçn"
    else:
        status_message = st.session_state.current_config_status
        
    st.markdown(f"**Tr·∫°ng th√°i:** *{status_message}*")

    # Y√™u c·∫ßu 2: ƒê·∫∑t n√∫t Kh·ªüi t·∫°o RAG ngay d∆∞·ªõi th√¥ng b√°o
    if st.button("**üöÄ Kh·ªüi t·∫°o RAG**", use_container_width=True):
        # C·∫≠p nh·∫≠t last_config bao g·ªìm c·∫£ show_sources
        current_show_sources = st.session_state.show_sources_checkbox 
        st.session_state.last_config = (temperature, k_value, search_mode, use_filter, current_show_sources) 
        
        # TRUY·ªÄN ƒê·ª¶ 5 THAM S·ªê
        initialize_rag_system(temperature, k_value, search_mode, use_filter, current_show_sources) 
        st.rerun()

    # B. L·ªãch s·ª≠ Chat (Gi·ªØ nguy√™n)
    st.markdown("---")
    if st.session_state.chat_history:
        st.markdown("### üí¨ L·ªãch s·ª≠ h·ªôi tho·∫°i")
        
        for chat in reversed(st.session_state.chat_history[-10:]):
            is_active = chat['id'] == st.session_state.current_chat_id
            
            col1, col2 = st.columns([4, 1])
            with col1:
                if st.button(f"üí≠ {chat['title']}", key=f"chat_{chat['id']}", use_container_width=True, type="primary" if is_active else "secondary"):
                    load_chat(chat['id'])
                    st.rerun()
            
            with col2:
                if st.button("üóëÔ∏è", key=f"del_{chat['id']}", help="X√≥a"):
                    delete_chat(chat['id'])
                    st.rerun()
            
            st.caption(f"üïê {chat['timestamp']}")
            # st.markdown("---") # B·ªè g·∫°ch ngang v√¨ caption ƒë√£ c√≥ kho·∫£ng tr·ªëng

    
    # D. Th·ªëng k√™ v√† X√≥a (Gi·ªØ nguy√™n)
    st.markdown("---")
    st.markdown("### üìä Th·ªëng k√™")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("C√¢u h·ªèi", st.session_state.total_questions)
    with col2:
        st.metric("H·ªôi tho·∫°i", len(st.session_state.chat_history))
    
    st.markdown("---")
    
    if st.button("üóëÔ∏è X√≥a T·∫•t c·∫£ D·ªØ li·ªáu", use_container_width=True):
        st.session_state.messages = []
        st.session_state.total_questions = 0
        st.session_state.chat_history = []
        st.session_state.current_chat_id = None
        st.session_state['rag_initialized'] = False 
        st.session_state.current_config_status = "Ch∆∞a kh·ªüi t·∫°o"
        st.rerun()
        
    st.markdown("---")
    st.info("""**3NHistory** s·ª≠ d·ª•ng Gemini AI, Weaviate v√† m√¥ h√¨nh Embedding c·ª•c b·ªô. Ph·ª•c v·ª• m·ª•c ƒë√≠ch gi√°o d·ª•c & nghi√™n c·ª©u.""")


# --- Top Bar (Gi·ªØ nguy√™n) ---
st.markdown(f"""
<div class="top-bar">
    <div class="top-bar-left">
        <img src="[YOUR_PUBLIC_LOGO_URL]" alt="3NHistory Logo" style="height: 40px;"/>
    </div>
    <div class="stats-badge">
        üí¨ {st.session_state.total_questions} c√¢u h·ªèi
    </div>
</div>
""", unsafe_allow_html=True)

# --- Main Chat Area ---
st.markdown('<div class="chat-container">', unsafe_allow_html=True)

# --- A. Hi·ªÉn th·ªã L·ªùi ch√†o / G·ª£i √Ω ---
if len(st.session_state.messages) == 0:
    # Welcome Screen
    st.markdown("""
    <div class="welcome-container">
        <div class="welcome-title">Ch√†o m·ª´ng ƒë·∫øn v·ªõi 3NHistory</div>
        <div class="welcome-subtitle">
            üáªüá≥ Tr·ª£ l√Ω AI chuy√™n v·ªÅ l·ªãch s·ª≠ Vi·ªát Nam (giai ƒëo·∫°n t·ª´ nƒÉm 1945 - 1975)<br>
            Kh√°m ph√° nh·ªØng trang s·ª≠ v√†ng c·ªßa d√¢n t·ªôc
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # Suggestion cards
    st.markdown('<div class="suggestion-grid">', unsafe_allow_html=True)
    cols = st.columns(4)
    suggestions = [
        {"icon": "‚öîÔ∏è", "title": "Chi·∫øn d·ªãch qu√¢n s·ª±", "text": "ƒêi·ªán Bi√™n Ph·ªß", "query": "K·ªÉ cho t√¥i nghe v·ªÅ chi·∫øn d·ªãch ƒêi·ªán Bi√™n Ph·ªß"},
        {"icon": "üèõÔ∏è", "title": "S·ª± ki·ªán ch√≠nh tr·ªã", "text": "Tuy√™n ng√¥n ƒë·ªôc l·∫≠p", "query": "Tuy√™n ng√¥n ƒë·ªôc l·∫≠p 1945 c√≥ √Ω nghƒ©a g√¨?"},
        {"icon": "ü§ù", "title": "Ngo·∫°i giao", "text": "H·ªôi ngh·ªã Geneva", "query": "H·ªôi ngh·ªã Geneva 1954 di·ªÖn ra nh∆∞ th·∫ø n√†o?"},
        {"icon": "üë•", "title": "Nh√¢n v·∫≠t l·ªãch s·ª≠", "text": "Vƒ© nh√¢n d√¢n t·ªôc", "query": "Vai tr√≤ c·ªßa H·ªì Ch√≠ Minh trong kh√°ng chi·∫øn"}
    ]
    
    for i, col in enumerate(cols):
        with col:
            if st.button(
                f"{suggestions[i]['icon']}\n\n**{suggestions[i]['title']}**\n\n{suggestions[i]['text']}", 
                key=f"suggest_{i}",
                use_container_width=True
            ):
                if not st.session_state['rag_initialized']:
                    st.warning("‚ö†Ô∏è Vui l√≤ng **Kh·ªüi t·∫°o H·ªá th·ªëng RAG** ·ªü sidebar tr∆∞·ªõc khi ƒë·∫∑t c√¢u h·ªèi!")
                else:
                    st.session_state.messages.append({"role": "user", "content": suggestions[i]['query']})
                    st.session_state.total_questions += 1
                    st.session_state.process_rag = True # K√≠ch ho·∫°t x·ª≠ l√Ω RAG
                    st.rerun() 

else:
    # --- B. Hi·ªÉn th·ªã Tin nh·∫Øn (L·∫ßn 1: Hi·ªÉn th·ªã User, L·∫ßn 2: Hi·ªÉn th·ªã Bot) ---
    for message in st.session_state.messages:
        avatar = "üë§" if message["role"] == "user" else "ü§ñ"
        with st.chat_message(message["role"], avatar=avatar):
            st.markdown(message["content"])
            
            # Ch·ªâ hi·ªÉn th·ªã ngu·ªìn n·∫øu l√† tin nh·∫Øn assistant v√† c·ªù show_sources ƒëang b·∫≠t
            if (message["role"] == "assistant" and 
                st.session_state.show_sources_checkbox and 
                "sources" in message and 
                message["sources"]):
                
                # Hi·ªÉn th·ªã th√¥ng tin l·ªçc
                if message.get('filter_info'):
                    st.info(message['filter_info'])
                
                # Hi·ªÉn th·ªã ngu·ªìn
                with st.expander("üìö Ngu·ªìn tham kh·∫£o"):
                    for i, doc in enumerate(message["sources"]):
                        period = doc.metadata.get('period', 'N/A')
                        content = doc.page_content[:200] + "..."
                        
                        st.markdown(f"""
                        <div class="source-card">
                            <strong>üìÑ Ngu·ªìn {i+1}</strong> 
                            <span style="color: #c4b5fd;">(Th·ªùi k·ª≥: {period})</span><br>
                            <em style="color: #cbd5e1; line-height: 1.6;">{content}</em>
                        </div>
                        """, unsafe_allow_html=True)
                        
    st.markdown('</div>', unsafe_allow_html=True)


# ----------------------------------------------------------------------
# 4. LOGIC X·ª¨ L√ù RAG RI√äNG BI·ªÜT (ƒê√£ T√°ch bi·ªát)
# ----------------------------------------------------------------------

if st.session_state.process_rag and st.session_state.rag_initialized:
    # Reset c·ªù ƒë·ªÉ kh√¥ng b·ªã l·∫∑p v√¥ h·∫°n
    st.session_state.process_rag = False 
    
    # L·∫•y c√¢u h·ªèi cu·ªëi c√πng c·ªßa ng∆∞·ªùi d√πng
    user_prompt = st.session_state.messages[-1]["content"]

    # Kh·ªëi x·ª≠ l√Ω RAG (S·∫Ω t·ª± ƒë·ªông hi·ªÉn th·ªã ph√≠a d∆∞·ªõi tin nh·∫Øn ng∆∞·ªùi d√πng v·ª´a ƒë∆∞·ª£c hi·ªÉn th·ªã)
    try:
        # S·ª≠ d·ª•ng st.chat_message("assistant") ƒë·ªÉ Streamlit bi·∫øt ƒë√¢y l√† tin nh·∫Øn bot
        with st.chat_message("assistant", avatar="ü§ñ"):
            
            # S·ª≠ d·ª•ng st.status ƒë·ªÉ th√¥ng b√°o ƒëang x·ª≠ l√Ω
            with st.status("üîç ƒêang tra c·ª©u t√†i li·ªáu l·ªãch s·ª≠...", expanded=True) as status:
                
                # G·ªçi RAG
                status.update(label="‚åõ ƒêang t·ªïng h·ª£p v√† ph√¢n t√≠ch th√¥ng tin...", state="running")
                final_answer, context_docs, filter_info = st.session_state['rag_retriever'].ask(user_prompt)
                status.update(label="‚úÖ ƒê√£ ho√†n th√†nh tra c·ª©u", state="complete", expanded=False)
            
            # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi cu·ªëi c√πng
            st.markdown(final_answer)
            
            # Chu·∫©n b·ªã tin nh·∫Øn bot ƒë·ªÉ l∆∞u v√†o session state
            bot_message = {
                "role": "assistant",
                "content": final_answer,
                "sources": context_docs,
                "filter_info": filter_info
            }
            
            # Hi·ªÉn th·ªã th√¥ng tin l·ªçc v√† ngu·ªìn tham kh·∫£o ngay t·∫°i ƒë√¢y
            if filter_info:
                st.info(filter_info) 
            
            if st.session_state.show_sources_checkbox and context_docs:
                with st.expander("üìö Ngu·ªìn tham kh·∫£o"):
                    for i, doc in enumerate(context_docs):
                        period = doc.metadata.get('period', 'N/A')
                        content = doc.page_content[:200] + "..."
                        st.markdown(f"""
                        <div class="source-card">
                            <strong>üìÑ Ngu·ªìn {i+1}</strong> 
                            <span style="color: #c4b5fd;">(Th·ªùi k·ª≥: {period})</span><br>
                            <em style="color: #cbd5e1; line-height: 1.6;">{content}</em>
                        </div>
                        """, unsafe_allow_html=True)

        # L∆∞u tin nh·∫Øn bot v√†o Session State (sau khi ƒë√£ hi·ªÉn th·ªã xong)
        st.session_state.messages.append(bot_message)
        
        # G·ªçi rerun cu·ªëi c√πng ƒë·ªÉ ƒë·∫£m b·∫£o UI s·∫°ch s·∫Ω (V√≠ d·ª•: x√≥a n·ªôi dung st.chat_input)
        st.rerun() 
        
    except Exception as e:
        error_msg = f"‚ùå ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh RAG: {str(e)}"
        st.error(error_msg)
        st.session_state.messages.append({
            "role": "assistant",
            "content": error_msg
        })
        st.rerun()


# ----------------------------------------------------------------------
# 5. INPUT CHAT SAU C√ôNG (Ch·ªâ d√πng ƒë·ªÉ th√™m tin nh·∫Øn v√† RERUN)
# ----------------------------------------------------------------------

if prompt := st.chat_input("üí≠ H·ªèi v·ªÅ l·ªãch s·ª≠ Vi·ªát Nam 1945-1975..."):
    # 1. Th√™m c√¢u h·ªèi ng∆∞·ªùi d√πng v√†o messages
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.session_state.total_questions += 1
    
    # 2. ƒê·∫∑t c·ªù ƒë·ªÉ k√≠ch ho·∫°t RAG trong l·∫ßn ch·∫°y l·∫°i (n·∫øu ƒë√£ kh·ªüi t·∫°o)
    if st.session_state['rag_initialized']:
        st.session_state.process_rag = True
    else:
        # N·∫øu RAG ch∆∞a kh·ªüi t·∫°o, th√™m tin nh·∫Øn l·ªói
        st.session_state.messages.append({
            "role": "assistant", 
            "content": "‚ùå Vui l√≤ng **Kh·ªüi t·∫°o H·ªá th·ªëng RAG** ·ªü sidebar tr∆∞·ªõc khi ƒë·∫∑t c√¢u h·ªèi!"
        })
        
    # 3. Y√™u c·∫ßu ch·∫°y l·∫°i ngay l·∫≠p t·ª©c ƒë·ªÉ hi·ªÉn th·ªã tin nh·∫Øn ng∆∞·ªùi d√πng (v√† b·∫Øt ƒë·∫ßu RAG n·∫øu c·ªù process_rag = True)
    st.rerun()